# ğŸ§  Deep Neural Network for Non-Linear Regression â€” 8 Implementations

> **3-Layer DNN across NumPy, PyTorch, PyTorch Lightning, and TensorFlow (4 variants)**
> 
> Every implementation solves the **same problem** using different frameworks and abstraction levels â€” from pure manual backpropagation to high-level `model.fit()`.

- Colab A: https://colab.research.google.com/drive/1UX6DCdDNB1FzSa-Q0dUVe7e6a4by6Y6y?usp=sharing
- Video A: https://www.loom.com/share/762936546c754ec483198aa9ac3ed6dd

- Colab B: https://colab.research.google.com/drive/1tpNbD7d1lInJSodivLIGoTSxYPI3W3OX?usp=sharing
- Video B: https://www.loom.com/share/664dc44dce3d45238acb8f8651437781

- Colab C: https://colab.research.google.com/drive/11EgTuxYXtMeelUyP-5hs0dXXm1Xpm5W1?usp=sharing
- Video C: https://www.loom.com/share/3a2e4917d66f4ea68cdb3773a234fbbb

- Colab D: https://colab.research.google.com/drive/1XljznxEpP3KS9_w7a7iKAby7KflqruqV?usp=sharing
- Video D: https://www.loom.com/share/d18ee5f3a5e6480cb543534631df708f

- Colab E-1: https://colab.research.google.com/drive/1nqmmkhcwEPkavuZYB8QxEuz7D21UHnY4?usp=sharing
- Video E-1: https://www.loom.com/share/0ee18247579f4682ab36f63cf01ce0e3

- Colab E-2: https://colab.research.google.com/drive/17Zb1eS5llzUC--xmbh1m3-8HD3D0_0wS?usp=sharing
- Video E-2: https://www.loom.com/share/0d4d7238b28c4843b7f8f1e2f4cf6d64

- Colab E-3: https://colab.research.google.com/drive/1Fme6RdIRg0ad02nSj5mUF1d75Fym5y7n?usp=sharing
- Video E-3: https://www.loom.com/share/e3ec44c544e84ebb91a5ae2bc62bedca

- Colab E-4: https://colab.research.google.com/drive/1qljkEX61f1FIiddr_fexlZrBjFvkUxPW?usp=sharing
- Video E-4: https://www.loom.com/share/8d66d1d93d5d4e4da1fc22972b8d24b4
---

## ğŸ¯ Problem Statement

All 8 notebooks solve **the same non-linear regression problem** with a **3-layer deep neural network**.

### Target Equation (3 Variables)

```
y = sin(xâ‚) Â· xâ‚‚Â² + cos(xâ‚ƒ) Â· xâ‚ + 0.5 Â· xâ‚ƒ Â· xâ‚‚
```

- **Inputs**: 3 variables (xâ‚, xâ‚‚, xâ‚ƒ) uniformly sampled from [-2, 2]
- **Output**: 1 continuous value (y)
- **Samples**: 2,000 data points with normalization

### Network Architecture

```
Input(3) â†’ Dense(64, ReLU) â†’ Dense(32, ReLU) â†’ Dense(16, ReLU) â†’ Dense(1, Linear)
```

| Layer | Neurons | Activation | Parameters |
|-------|---------|------------|------------|
| Input | 3 | â€” | â€” |
| Hidden 1 | 64 | ReLU | 256 (W) + 64 (b) = 320 |
| Hidden 2 | 32 | ReLU | 2,048 (W) + 32 (b) = 2,080 |
| Hidden 3 | 16 | ReLU | 512 (W) + 16 (b) = 528 |
| Output | 1 | Linear | 16 (W) + 1 (b) = 17 |
| **Total** | | | **2,945** |

### 4D Visualization

Since we have 3 input features + 1 output (4D), we use **PCA** (scikit-learn) to reduce input dimensionality to 2 components, then visualize with the target as color/z-axis.

---

## ğŸ“ Repository Structure

```
â”œâ”€â”€ README.md                                    â† This file
â”œâ”€â”€ VIDEO_SCRIPTS.md                             â† All 8 video narration scripts
â”œâ”€â”€ Colab_A_NumPy_From_Scratch_3Layer_DNN.ipynb  â† (a) NumPy + tf.einsum manual backprop
â”œâ”€â”€ Colab_B_PyTorch_From_Scratch_3Layer_DNN.ipynbâ† (b) PyTorch raw tensors, no nn.Module
â”œâ”€â”€ Colab_C_PyTorch_Classes_3Layer_DNN.ipynb     â† (c) PyTorch nn.Module class-based
â”œâ”€â”€ Colab_D_PyTorch_Lightning_3Layer_DNN.ipynb   â† (d) PyTorch Lightning
â”œâ”€â”€ Colab_Ei_TF_From_Scratch_LowLevel.ipynb      â† (e-i) TF low-level, no Keras
â”œâ”€â”€ Colab_Eii_TF_BuiltIn_Layers.ipynb            â† (e-ii) TF Model subclassing + Dense
â”œâ”€â”€ Colab_Eiii_TF_Functional_API.ipynb            â† (e-iii) TF Functional API
â””â”€â”€ Colab_Eiv_TF_HighLevel_Sequential.ipynb       â† (e-iv) TF Sequential + model.fit
```

---

## ğŸ““ Colab Notebooks & Videos

### Colab A â€” NumPy From Scratch (Manual Backprop)

| | |
|---|---|
| **File** | [`Colab_A_NumPy_From_Scratch_3Layer_DNN.ipynb`](Colab_A_NumPy_From_Scratch_3Layer_DNN.ipynb) |
| **Framework** | NumPy + `tf.einsum` (TF used ONLY for einsum) |
| **Abstraction** | â­ Lowest â€” everything manual |
| **Video** | ğŸ“¹ [Watch Walkthrough](#video-a) |

**What makes this unique:**
- **Manual backpropagation** â€” chain rule gradient computation coded by hand
- **`tf.einsum('ij,jk->ik', ...)`** replaces all `np.matmul` / `np.dot` calls (assignment requirement)
- Forward pass caches all intermediate Z and A values for backprop
- He initialization (`âˆš(2/fan_in)`) for ReLU compatibility
- Mini-batch gradient descent with shuffling

**Key Cells:**
| Cell | Description |
|------|------------|
| Cell 2 | Generates synthetic data from the 3-variable non-linear equation |
| Cell 3 | 4D visualization using PCA dimensionality reduction |
| Cell 4 | Network architecture definition & He weight initialization |
| Cell 6 | **Forward pass with `tf.einsum`** â€” the core assignment requirement |
| Cell 8 | **Manual backward pass** â€” chain rule through all 4 layers |
| Cell 10 | Training loop (200 epochs, batch size 64) |
| Cell 11 | Loss curve (log scale) |
| Cell 12 | Predicted vs actual, residual histogram, PCA prediction plot |

---

### Colab B â€” PyTorch From Scratch (No nn.Module)

| | |
|---|---|
| **File** | [`Colab_B_PyTorch_From_Scratch_3Layer_DNN.ipynb`](Colab_B_PyTorch_From_Scratch_3Layer_DNN.ipynb) |
| **Framework** | PyTorch (raw tensors only) |
| **Abstraction** | â­ Low â€” no built-in layer classes |
| **Video** | ğŸ“¹ [Watch Walkthrough](#video-b) |

**What makes this unique:**
- **NO `nn.Module`, NO `nn.Linear`, NO optimizer object**
- Weights are raw `torch.Tensor` with `requires_grad=True`
- Forward pass uses `@` operator for matrix multiply
- PyTorch autograd computes backward pass, but SGD update is manual
- `p -= learning_rate * p.grad` inside `torch.no_grad()` block

**Key Cells:**
| Cell | Description |
|------|------------|
| Cell 4 | Raw tensor weight initialization with `requires_grad_(True)` |
| Cell 5 | Forward pass â€” pure `@` operator + `torch.relu`, no layer classes |
| Cell 6 | Training with `loss.backward()` + **manual SGD** (no `optim.SGD`) |

---

### Colab C â€” PyTorch nn.Module (Class-Based)

| | |
|---|---|
| **File** | [`Colab_C_PyTorch_Classes_3Layer_DNN.ipynb`](Colab_C_PyTorch_Classes_3Layer_DNN.ipynb) |
| **Framework** | PyTorch (standard nn.Module) |
| **Abstraction** | â­â­ Medium â€” standard PyTorch practice |
| **Video** | ğŸ“¹ [Watch Walkthrough](#video-c) |

**What makes this unique:**
- Standard **`nn.Module` subclassing** â€” the PyTorch recommended approach
- `nn.Sequential` with `nn.Linear` + `nn.ReLU`
- Kaiming/He initialization via `nn.init.kaiming_normal_`
- `torch.optim.Adam` optimizer with `nn.MSELoss`
- Train/test split with `DataLoader`

**Key Cells:**
| Cell | Description |
|------|------------|
| Cell 4 | `NonLinearRegressionNet(nn.Module)` class definition |
| Cell 5 | `nn.MSELoss()` + `optim.Adam()` |
| Cell 6 | Standard train/eval loop with `model.train()` / `model.eval()` |

---

### Colab D â€” PyTorch Lightning

| | |
|---|---|
| **File** | [`Colab_D_PyTorch_Lightning_3Layer_DNN.ipynb`](Colab_D_PyTorch_Lightning_3Layer_DNN.ipynb) |
| **Framework** | PyTorch Lightning |
| **Abstraction** | â­â­â­ High â€” framework handles boilerplate |
| **Video** | ğŸ“¹ [Watch Walkthrough](#video-d) |

**What makes this unique:**
- **`LightningModule`** â€” defines model + training/validation steps
- **`LightningDataModule`** â€” handles data loading pipeline
- **`Trainer`** â€” single line `trainer.fit(model, dm)` runs entire training
- Automatic device management (CPU/GPU), logging, and progress bars
- `save_hyperparameters()` for reproducibility

**Key Cells:**
| Cell | Description |
|------|------------|
| Cell 4 | `RegressionDataModule(pl.LightningDataModule)` |
| Cell 5 | `LitRegressionNet(pl.LightningModule)` with `training_step`, `configure_optimizers` |
| Cell 6 | `Trainer(max_epochs=200).fit(model, dm)` â€” one line training |

---

### Colab E(i) â€” TensorFlow Low-Level (No Keras)

| | |
|---|---|
| **File** | [`Colab_Ei_TF_From_Scratch_LowLevel.ipynb`](Colab_Ei_TF_From_Scratch_LowLevel.ipynb) |
| **Framework** | TensorFlow (raw `tf.Variable` + `tf.GradientTape`) |
| **Abstraction** | â­ Lowest TF level â€” no Keras at all |
| **Video** | ğŸ“¹ [Watch Walkthrough](#video-ei) |

**What makes this unique:**
- **NO Keras layers, NO Keras Model, NO optimizer object**
- Raw `tf.Variable` for all weights
- **`tf.einsum`** for matrix multiplications
- `tf.GradientTape` for automatic differentiation
- Manual SGD: `w.assign_sub(lr * gradient)`

**Key Cells:**
| Cell | Description |
|------|------------|
| Cell 4 | `tf.Variable(he_init([...]))` â€” raw weight creation |
| Cell 5 | Forward pass with `tf.einsum('ij,jk->ik', ...)` |
| Cell 6 | `tf.GradientTape` + manual `assign_sub` update |

---

### Colab E(ii) â€” TensorFlow Built-in Layers (Model Subclassing)

| | |
|---|---|
| **File** | [`Colab_Eii_TF_BuiltIn_Layers.ipynb`](Colab_Eii_TF_BuiltIn_Layers.ipynb) |
| **Framework** | TensorFlow / Keras (Model subclassing) |
| **Abstraction** | â­â­ Medium â€” built-in layers + custom loop |
| **Video** | ğŸ“¹ [Watch Walkthrough](#video-eii) |

**What makes this unique:**
- **`keras.Model` subclassing** â€” define architecture in `__init__`, logic in `call`
- `layers.Dense` handles weight creation, initialization, and forward math
- Custom training loop with `tf.GradientTape` (NOT `model.fit`)
- `@tf.function` decorator compiles to static graph for speed
- `keras.optimizers.Adam` for parameter updates

**Key Cells:**
| Cell | Description |
|------|------------|
| Cell 4 | `RegressionDNN(keras.Model)` with `layers.Dense` |
| Cell 5 | `@tf.function` decorated `train_step` with `GradientTape` |

---

### Colab E(iii) â€” TensorFlow Functional API

| | |
|---|---|
| **File** | [`Colab_Eiii_TF_Functional_API.ipynb`](Colab_Eiii_TF_Functional_API.ipynb) |
| **Framework** | TensorFlow / Keras (Functional API) |
| **Abstraction** | â­â­â­ High â€” declarative graph construction |
| **Video** | ğŸ“¹ [Watch Walkthrough](#video-eiii) |

**What makes this unique:**
- **`keras.Input(shape=(3,))`** declares input type
- Layers chained **functionally**: `x = Dense(64, 'relu')(inputs)` â†’ `x = Dense(32, 'relu')(x)` â†’ ...
- **`keras.Model(inputs, outputs)`** builds the model from the DAG
- Supports `plot_model()` for architecture visualization
- Enables multi-input / multi-output architectures (not possible with Sequential)

**Key Cells:**
| Cell | Description |
|------|------------|
| Cell 4 | Functional model: `keras.Input` â†’ chained `Dense` calls â†’ `keras.Model` |
| Cell 5 | `keras.utils.plot_model` â€” visual DAG of the network |

---

### Colab E(iv) â€” TensorFlow High-Level Sequential + `model.fit`

| | |
|---|---|
| **File** | [`Colab_Eiv_TF_HighLevel_Sequential.ipynb`](Colab_Eiv_TF_HighLevel_Sequential.ipynb) |
| **Framework** | TensorFlow / Keras (Sequential + fit) |
| **Abstraction** | â­â­â­â­ Highest â€” maximum automation |
| **Video** | ğŸ“¹ [Watch Walkthrough](#video-eiv) |

**What makes this unique:**
- **`keras.Sequential([...])` â€” model in ~6 lines**
- **`model.compile(optimizer, loss, metrics)`** â€” configure everything
- **`model.fit(X, Y, validation_data, callbacks)`** â€” one line training
- **`model.evaluate(X_test, Y_test)`** â€” one line testing
- **Callbacks**: `EarlyStopping` (patience=20), `ReduceLROnPlateau` (factor=0.5)
- Train/Val/Test split (70/15/15)

**Key Cells:**
| Cell | Description |
|------|------------|
| Cell 4 | `keras.Sequential` model definition |
| Cell 5 | `model.compile()` with optimizer, loss, metrics |
| Cell 6 | EarlyStopping + ReduceLROnPlateau callbacks |
| Cell 7 | `model.fit()` with full history tracking |

---

## ğŸ“Š Framework Comparison

| Aspect | A (NumPy) | B (PyTorch Raw) | C (PyTorch Module) | D (Lightning) | E-i (TF Low) | E-ii (TF Layers) | E-iii (TF Func) | E-iv (TF Seq) |
|--------|-----------|-----------------|--------------------|----|---|---|---|---|
| **Backprop** | Manual chain rule | Autograd | Autograd | Autograd | GradientTape | GradientTape | GradientTape | Automatic |
| **Weights** | NumPy arrays | Raw tensors | nn.Linear | nn.Linear | tf.Variable | Dense layers | Dense layers | Dense layers |
| **Optimizer** | Manual SGD | Manual SGD | optim.Adam | configure_optimizers | Manual SGD | Adam | Adam | Adam |
| **Training Loop** | Manual | Manual | Manual | Trainer | Manual | Custom | Custom | model.fit |
| **Matrix Multiply** | tf.einsum | @ operator | nn.Linear | nn.Linear | tf.einsum | Dense | Dense | Dense |
| **Lines of Code** | ~120 | ~80 | ~60 | ~50 | ~80 | ~60 | ~55 | ~30 |
| **Abstraction** | â­ | â­ | â­â­ | â­â­â­ | â­ | â­â­ | â­â­â­ | â­â­â­â­ |

### Abstraction Spectrum

```
LOWEST                                                              HIGHEST
   â”‚                                                                    â”‚
   â–¼                                                                    â–¼
Colab A â”€â”€â–º Colab B â”€â”€â–º Colab E(i) â”€â”€â–º Colab C â”€â”€â–º Colab E(ii) â”€â”€â–º Colab E(iii) â”€â”€â–º Colab D â”€â”€â–º Colab E(iv)
NumPy       PyTorch     TF Low         PyTorch     TF Layers       TF Functional    Lightning   TF Sequential
Manual BP   Raw Tensors Raw Variables  nn.Module   Subclassing     Functional API   auto loop   model.fit()
```

---

## ğŸš€ How to Run

### Option 1: Google Colab (Recommended)
1. Open any `.ipynb` file in this repository
2. Click "Open in Colab" or upload to [colab.research.google.com](https://colab.research.google.com)
3. Run all cells top to bottom (`Runtime â†’ Run all`)
4. No additional dependencies needed â€” all notebooks are self-contained

### Option 2: Local Jupyter
```bash
pip install numpy tensorflow torch pytorch-lightning scikit-learn matplotlib
jupyter notebook
```

### Dependencies
| Package | Version | Used In |
|---------|---------|---------|
| NumPy | â‰¥1.21 | All |
| TensorFlow | â‰¥2.10 | A, E(i-iv) |
| PyTorch | â‰¥1.12 | B, C, D |
| PyTorch Lightning | â‰¥1.9 | D |
| scikit-learn | â‰¥1.0 | All (PCA) |
| matplotlib | â‰¥3.5 | All |

---

## ğŸ¬ Video Walkthroughs

> Each video is a 2-3 minute screen recording walkthrough covering every cell in the notebook with code explanation and output demonstration.

| # | Notebook | Video Link | Duration |
|---|----------|-----------|----------|
| 1 | Colab A â€” NumPy From Scratch | <a name="video-a"></a> ğŸ“¹ [YouTube Link - REPLACE WITH YOUR URL] | ~2.5 min |
| 2 | Colab B â€” PyTorch From Scratch | <a name="video-b"></a> ğŸ“¹ [YouTube Link - REPLACE WITH YOUR URL] | ~2 min |
| 3 | Colab C â€” PyTorch nn.Module | <a name="video-c"></a> ğŸ“¹ [YouTube Link - REPLACE WITH YOUR URL] | ~2 min |
| 4 | Colab D â€” PyTorch Lightning | <a name="video-d"></a> ğŸ“¹ [YouTube Link - REPLACE WITH YOUR URL] | ~2 min |
| 5 | Colab E(i) â€” TF Low-Level | <a name="video-ei"></a> ğŸ“¹ [YouTube Link - REPLACE WITH YOUR URL] | ~2.5 min |
| 6 | Colab E(ii) â€” TF Built-in Layers | <a name="video-eii"></a> ğŸ“¹ [YouTube Link - REPLACE WITH YOUR URL] | ~2 min |
| 7 | Colab E(iii) â€” TF Functional API | <a name="video-eiii"></a> ğŸ“¹ [YouTube Link - REPLACE WITH YOUR URL] | ~2 min |
| 8 | Colab E(iv) â€” TF High-Level | <a name="video-eiv"></a> ğŸ“¹ [YouTube Link - REPLACE WITH YOUR URL] | ~2.5 min |

> **âš ï¸ Replace the video links above with your actual YouTube/Google Drive URLs after recording!**

### What Each Video Covers
- âœ… GitHub repository shown with all files checked in
- âœ… Cell-by-cell walkthrough of the executed Colab
- âœ… Explanation of code logic in each section
- âœ… Training output and loss curves
- âœ… Final predictions, RÂ² scores, and visualizations

---

## ğŸ“š References

- [TensorFlow 2.0 + Keras Crash Course (FranÃ§ois Chollet)](https://colab.research.google.com/drive/1UCJt8EYjlzCs1H1d1X0iDGYJsHKwu-NO)
- [Intro to Keras for Researchers](https://keras.io/getting_started/intro_to_keras_for_researchers/)
- [PyTorch Lightning Documentation](https://lightning.ai/docs/pytorch/stable/)
- [4D Plotting with Matplotlib](https://www.tutorialspoint.com/how-to-make-a-4d-plot-with-matplotlib-using-arbitrary-data)

---

*Built for CMPE/DATA 255 â€” Deep Learning Assignment*
